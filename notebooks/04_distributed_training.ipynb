{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Distributed Training - Multi-GPU Jobs\n",
        "\n",
        "## Distributed Training Overview\n",
        "\n",
        "### Single GPU vs Multi-GPU\n",
        "\n",
        "| Aspect | Single GPU | 4 GPUs (DDP) | 8 GPUs (DDP) |\n",
        "|--------|-----------|--------------|---------------|\n",
        "| **Batch Size** | 32 | 128 (32√ó4) | 256 (32√ó8) |\n",
        "| **Training Time** | 10 hours | ~2.5 hours | ~1.25 hours |\n",
        "| **Speedup** | 1√ó | ~4√ó | ~8√ó |\n",
        "| **Memory** | 16GB | 64GB total | 128GB total |\n",
        "\n",
        "### When to Use Multi-GPU\n",
        "- ‚úÖ Large datasets (>100K samples)\n",
        "- ‚úÖ Large models (>100M parameters)\n",
        "- ‚úÖ Production training (time-critical)\n",
        "- ‚úÖ Hyperparameter sweeps (parallel experiments)\n",
        "- ‚ùå Small experiments (<1 hour on 1 GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:09:53 - root - \u001b[32mINFO\u001b[0m - Logging initialized at level INFO [logger.py:202]\n",
            "2025-10-09 21:09:53 - src.resources.gpu_manager - \u001b[33mWARNING\u001b[0m - Failed to initialize pynvml: NVML Shared Library Not Found [gpu_manager.py:78]\n",
            "2025-10-09 21:09:53 - src.resources.gpu_manager - \u001b[33mWARNING\u001b[0m - No CUDA-capable GPUs detected [gpu_manager.py:94]\n",
            "‚úì Setup complete!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "from src.scheduler.job_queue import JobConfig, get_job_queue, Priority\n",
        "from src.resources.resource_pool import ResourcePoolManager, PoolType\n",
        "from src.resources.gpu_manager import get_gpu_manager\n",
        "from src.monitoring.logger import setup_logging\n",
        "\n",
        "setup_logging(level=\"INFO\")\n",
        "\n",
        "# Create resource pool manager instance\n",
        "resource_pool_manager = ResourcePoolManager()\n",
        "\n",
        "print(\"‚úì Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Multi-GPU Resource Allocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total GPUs: 0\n"
          ]
        }
      ],
      "source": [
        "# Check available GPUs\n",
        "gpu_manager = get_gpu_manager()\n",
        "pool_manager = resource_pool_manager\n",
        "\n",
        "print(f\"Total GPUs: {gpu_manager.num_gpus}\")\n",
        "\n",
        "# Check production pool capacity\n",
        "prod_pool = pool_manager.get_pool(PoolType.PRODUCTION)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Distributed Training Job Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distributed Training Configuration:\n",
            "  GPUs: 4\n",
            "  Backend: nccl\n",
            "  Per-GPU Batch: 32\n",
            "  Total Batch: 128\n",
            "  With Grad Accum: 256\n",
            "  Mixed Precision: True\n"
          ]
        }
      ],
      "source": [
        "# Configure 4-GPU distributed training job\n",
        "distributed_job = JobConfig(\n",
        "    job_id=\"distributed-training-001\",\n",
        "    user_id=\"ml-team\",\n",
        "    job_type=\"fine_tuning\",\n",
        "    \n",
        "    # Multi-GPU settings\n",
        "    pool_type=\"production\",\n",
        "    num_gpus=4,  # Request 4 GPUs\n",
        "    is_preemptible=False,  # Don't interrupt\n",
        "    \n",
        "    # Model and data\n",
        "    model_name=\"bert-large-uncased\",  # Large model\n",
        "    dataset_path=\"./data/large_dataset.csv\",\n",
        "    output_dir=\"./output/distributed/run_001\",\n",
        "    \n",
        "    priority=\"MEDIUM\",\n",
        "    estimated_duration=7200,  # 2 hours\n",
        "    \n",
        "    config={\n",
        "        # Distributed training config\n",
        "        \"distributed\": True,\n",
        "        \"backend\": \"nccl\",  # NVIDIA Collective Communications Library\n",
        "        \"world_size\": 4,  # 4 GPUs\n",
        "        \n",
        "        # Per-GPU batch size (total = 32 √ó 4 = 128)\n",
        "        \"per_device_train_batch_size\": 32,\n",
        "        \"per_device_eval_batch_size\": 32,\n",
        "        \n",
        "        # Optimization\n",
        "        \"gradient_accumulation_steps\": 2,  # Effective batch = 128 √ó 2 = 256\n",
        "        \"fp16\": True,  # Mixed precision for speed\n",
        "        \n",
        "        # Training\n",
        "        \"learning_rate\": 3e-5,\n",
        "        \"num_train_epochs\": 3,\n",
        "        \"max_seq_length\": 512,\n",
        "        \n",
        "        # Checkpointing\n",
        "        \"save_steps\": 1000,\n",
        "        \"save_total_limit\": 3,\n",
        "        \"eval_steps\": 500,\n",
        "        \n",
        "        # Distributed-specific\n",
        "        \"ddp_find_unused_parameters\": False,\n",
        "        \"dataloader_num_workers\": 4,\n",
        "        \"dataloader_pin_memory\": True\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Distributed Training Configuration:\")\n",
        "print(f\"  GPUs: {distributed_job.num_gpus}\")\n",
        "print(f\"  Backend: {distributed_job.config['backend']}\")\n",
        "print(f\"  Per-GPU Batch: {distributed_job.config['per_device_train_batch_size']}\")\n",
        "print(f\"  Total Batch: {distributed_job.num_gpus * distributed_job.config['per_device_train_batch_size']}\")\n",
        "print(f\"  With Grad Accum: {distributed_job.num_gpus * distributed_job.config['per_device_train_batch_size'] * distributed_job.config['gradient_accumulation_steps']}\")\n",
        "print(f\"  Mixed Precision: {distributed_job.config['fp16']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. PyTorch DistributedDataParallel (DDP) Setup\n",
        "\n",
        "This is how the platform sets up distributed training internally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Internal DDP Setup (PyTorch):\n",
            "\n",
            "```python\n",
            "import torch\n",
            "import torch.distributed as dist\n",
            "from torch.nn.parallel import DistributedDataParallel as DDP\n",
            "\n",
            "# Initialize process group\n",
            "dist.init_process_group(\n",
            "    backend='nccl',\n",
            "    init_method='env://',  # Uses MASTER_ADDR, MASTER_PORT\n",
            "    world_size=4,          # 4 GPUs\n",
            "    rank=local_rank         # 0, 1, 2, 3\n",
            ")\n",
            "\n",
            "# Set device for this process\n",
            "torch.cuda.set_device(local_rank)\n",
            "device = torch.device(f'cuda:{local_rank}')\n",
            "\n",
            "# Wrap model with DDP\n",
            "model = create_model()\n",
            "model = model.to(device)\n",
            "model = DDP(model, device_ids=[local_rank])\n",
            "\n",
            "# Create distributed sampler\n",
            "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
            "    train_dataset,\n",
            "    num_replicas=4,\n",
            "    rank=local_rank\n",
            ")\n",
            "\n",
            "# DataLoader with distributed sampler\n",
            "train_loader = torch.utils.data.DataLoader(\n",
            "    train_dataset,\n",
            "    batch_size=32,  # Per-GPU batch\n",
            "    sampler=train_sampler,\n",
            "    num_workers=4,\n",
            "    pin_memory=True\n",
            ")\n",
            "\n",
            "# Training loop\n",
            "for epoch in range(num_epochs):\n",
            "    train_sampler.set_epoch(epoch)  # Shuffle differently each epoch\n",
            "    \n",
            "    for batch in train_loader:\n",
            "        # Each GPU processes different batch\n",
            "        outputs = model(batch)\n",
            "        loss = compute_loss(outputs)\n",
            "        \n",
            "        loss.backward()\n",
            "        # Gradients averaged across GPUs automatically\n",
            "        \n",
            "        optimizer.step()\n",
            "        optimizer.zero_grad()\n",
            "```\n",
            "\n",
            "Key Points:\n",
            "- Each GPU runs separate process (rank 0-3)\n",
            "- Data split across GPUs (DistributedSampler)\n",
            "- Gradients synchronized via all-reduce\n",
            "- Only rank 0 saves checkpoints\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example DDP setup code (for reference)\n",
        "print(\"\"\"\n",
        "Internal DDP Setup (PyTorch):\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "# Initialize process group\n",
        "dist.init_process_group(\n",
        "    backend='nccl',\n",
        "    init_method='env://',  # Uses MASTER_ADDR, MASTER_PORT\n",
        "    world_size=4,          # 4 GPUs\n",
        "    rank=local_rank         # 0, 1, 2, 3\n",
        ")\n",
        "\n",
        "# Set device for this process\n",
        "torch.cuda.set_device(local_rank)\n",
        "device = torch.device(f'cuda:{local_rank}')\n",
        "\n",
        "# Wrap model with DDP\n",
        "model = create_model()\n",
        "model = model.to(device)\n",
        "model = DDP(model, device_ids=[local_rank])\n",
        "\n",
        "# Create distributed sampler\n",
        "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    train_dataset,\n",
        "    num_replicas=4,\n",
        "    rank=local_rank\n",
        ")\n",
        "\n",
        "# DataLoader with distributed sampler\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,  # Per-GPU batch\n",
        "    sampler=train_sampler,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    train_sampler.set_epoch(epoch)  # Shuffle differently each epoch\n",
        "    \n",
        "    for batch in train_loader:\n",
        "        # Each GPU processes different batch\n",
        "        outputs = model(batch)\n",
        "        loss = compute_loss(outputs)\n",
        "        \n",
        "        loss.backward()\n",
        "        # Gradients averaged across GPUs automatically\n",
        "        \n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "```\n",
        "\n",
        "Key Points:\n",
        "- Each GPU runs separate process (rank 0-3)\n",
        "- Data split across GPUs (DistributedSampler)\n",
        "- Gradients synchronized via all-reduce\n",
        "- Only rank 0 saves checkpoints\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Submit Distributed Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:17:02 - src.scheduler.priority_manager - \u001b[33mWARNING\u001b[0m - Job distributed-001 already in queue [priority_manager.py:116]\n",
            "2025-10-09 21:17:02 - src.scheduler.job_queue - \u001b[32mINFO\u001b[0m - Submitted job distributed-001 to queue 'default' (priority=MEDIUM) [job_queue.py:340]\n",
            "‚úì Distributed job submitted: distributed-001\n",
            "  Waiting for 4 GPUs...\n",
            "  Job will start when resources available\n"
          ]
        }
      ],
      "source": [
        "job_queue = get_job_queue()\n",
        "\n",
        "try:\n",
        "    job_id = job_queue.submit_job(distributed_job, Priority.MEDIUM)\n",
        "    print(f\"‚úì Distributed job submitted: {job_id}\")\n",
        "    print(f\"  Waiting for 4 GPUs...\")\n",
        "    print(f\"  Job will start when resources available\")\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Submission failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Performance Optimization Tips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Distributed Training Optimization:\n",
            "\n",
            "1. **Batch Size Scaling**\n",
            "   - Scale batch size linearly with GPUs\n",
            "   - 1 GPU: batch=32 ‚Üí 4 GPUs: batch=128\n",
            "   - Adjust learning rate: LR √ó sqrt(N_GPUs)\n",
            "\n",
            "2. **Gradient Accumulation**\n",
            "   - Simulate larger batches\n",
            "   - 4 GPUs √ó batch 32 √ó accum 2 = effective 256\n",
            "   - Useful when GPU memory limited\n",
            "\n",
            "3. **Mixed Precision (FP16)**\n",
            "   - 2√ó faster training\n",
            "   - 2√ó less memory\n",
            "   - Enable with: fp16=True\n",
            "   - Uses NVIDIA Apex/AMP\n",
            "\n",
            "4. **Data Loading**\n",
            "   - num_workers=4 per GPU\n",
            "   - pin_memory=True for faster GPU transfer\n",
            "   - Prefetch data during GPU compute\n",
            "\n",
            "5. **NCCL Backend**\n",
            "   - Fastest for NVIDIA GPUs\n",
            "   - Optimized all-reduce\n",
            "   - Requires CUDA-aware MPI\n",
            "\n",
            "6. **Gradient Checkpointing**\n",
            "   - Save memory for large models\n",
            "   - Trade compute for memory\n",
            "   - Enable with: gradient_checkpointing=True\n",
            "\n",
            "7. **Communication Optimization**\n",
            "   - ddp_find_unused_parameters=False (faster)\n",
            "   - Bucket gradients for fewer syncs\n",
            "   - Overlap compute and communication\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\"\"\n",
        "üöÄ Distributed Training Optimization:\n",
        "\n",
        "1. **Batch Size Scaling**\n",
        "   - Scale batch size linearly with GPUs\n",
        "   - 1 GPU: batch=32 ‚Üí 4 GPUs: batch=128\n",
        "   - Adjust learning rate: LR √ó sqrt(N_GPUs)\n",
        "\n",
        "2. **Gradient Accumulation**\n",
        "   - Simulate larger batches\n",
        "   - 4 GPUs √ó batch 32 √ó accum 2 = effective 256\n",
        "   - Useful when GPU memory limited\n",
        "\n",
        "3. **Mixed Precision (FP16)**\n",
        "   - 2√ó faster training\n",
        "   - 2√ó less memory\n",
        "   - Enable with: fp16=True\n",
        "   - Uses NVIDIA Apex/AMP\n",
        "\n",
        "4. **Data Loading**\n",
        "   - num_workers=4 per GPU\n",
        "   - pin_memory=True for faster GPU transfer\n",
        "   - Prefetch data during GPU compute\n",
        "\n",
        "5. **NCCL Backend**\n",
        "   - Fastest for NVIDIA GPUs\n",
        "   - Optimized all-reduce\n",
        "   - Requires CUDA-aware MPI\n",
        "\n",
        "6. **Gradient Checkpointing**\n",
        "   - Save memory for large models\n",
        "   - Trade compute for memory\n",
        "   - Enable with: gradient_checkpointing=True\n",
        "\n",
        "7. **Communication Optimization**\n",
        "   - ddp_find_unused_parameters=False (faster)\n",
        "   - Bucket gradients for fewer syncs\n",
        "   - Overlap compute and communication\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Monitoring Distributed Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fewer than 4 GPUs available\n"
          ]
        }
      ],
      "source": [
        "# Monitor all GPUs in distributed job\n",
        "if gpu_manager.num_gpus >= 4:\n",
        "    print(\"GPU Utilization (All 4 GPUs):\\n\")\n",
        "    \n",
        "    for gpu_id in range(min(4, gpu_manager.num_gpus)):\n",
        "        gpu_info = gpu_manager.get_gpu_info(gpu_id)\n",
        "        if gpu_info:\n",
        "            print(f\"GPU {gpu_id}:\")\n",
        "            print(f\"  Utilization: {gpu_info.utilization:.1f}%\")\n",
        "            print(f\"  Memory: {gpu_info.used_memory/(1024**3):.1f}GB / {gpu_info.total_memory/(1024**3):.1f}GB\")\n",
        "            print(f\"  Temperature: {gpu_info.temperature:.1f}¬∞C\\n\")\n",
        "    \n",
        "    print(\"\\n‚úì All GPUs should show similar utilization (~95-100%)\")\n",
        "    print(\"  If one GPU is low, check for data loading bottleneck\")\n",
        "else:\n",
        "    print(\"Fewer than 4 GPUs available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Scaling Comparison\n",
        "\n",
        "### Example: BERT-Large Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT-Large Fine-tuning Scaling:\n",
            "\n",
            " GPUs  Batch  Time (hours)  Speedup  Efficiency  Time (hours\n",
            "    1     16          10.0     1.00       100.0          NaN\n",
            "    2     32           5.2     1.92        96.0          NaN\n",
            "    4     64           NaN     3.85        96.2          2.6\n",
            "    8    128           1.4     7.14        89.3          NaN\n",
            "\n",
            "üìä Analysis:\n",
            "  - Near-linear speedup up to 4 GPUs (96% efficiency)\n",
            "  - 8 GPUs: 7.14√ó speedup (89% efficiency)\n",
            "  - Efficiency loss due to communication overhead\n",
            "  - Sweet spot: 4 GPUs for most use cases\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Scaling efficiency data\n",
        "scaling_data = pd.DataFrame([\n",
        "    {\"GPUs\": 1, \"Batch\": 16, \"Time (hours)\": 10.0, \"Speedup\": 1.0, \"Efficiency\": 100.0},\n",
        "    {\"GPUs\": 2, \"Batch\": 32, \"Time (hours)\": 5.2, \"Speedup\": 1.92, \"Efficiency\": 96.0},\n",
        "    {\"GPUs\": 4, \"Batch\": 64, \"Time (hours\": 2.6, \"Speedup\": 3.85, \"Efficiency\": 96.2},\n",
        "    {\"GPUs\": 8, \"Batch\": 128, \"Time (hours)\": 1.4, \"Speedup\": 7.14, \"Efficiency\": 89.3},\n",
        "])\n",
        "\n",
        "print(\"BERT-Large Fine-tuning Scaling:\\n\")\n",
        "print(scaling_data.to_string(index=False))\n",
        "\n",
        "print(\"\\nüìä Analysis:\")\n",
        "print(\"  - Near-linear speedup up to 4 GPUs (96% efficiency)\")\n",
        "print(\"  - 8 GPUs: 7.14√ó speedup (89% efficiency)\")\n",
        "print(\"  - Efficiency loss due to communication overhead\")\n",
        "print(\"  - Sweet spot: 4 GPUs for most use cases\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
