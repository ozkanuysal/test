{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Monitoring & Observability Tutorial\n",
        "\n",
        "\n",
        "\n",
        "## Metrics Categories\n",
        "\n",
        "### 1. GPU Metrics\n",
        "- `ml_platform_gpu_utilization_percent`\n",
        "- `ml_platform_gpu_temperature_celsius`\n",
        "- `ml_platform_gpu_memory_used_bytes`\n",
        "- `ml_platform_gpu_memory_total_bytes`\n",
        "\n",
        "### 2. Job Metrics\n",
        "- `ml_platform_jobs_submitted_total`\n",
        "- `ml_platform_jobs_completed_total`\n",
        "- `ml_platform_jobs_failed_total`\n",
        "- `ml_platform_job_duration_seconds`\n",
        "\n",
        "### 3. Queue Metrics\n",
        "- `ml_platform_queue_depth`\n",
        "- `ml_platform_active_workers_total`\n",
        "- `ml_platform_queue_wait_time_seconds`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:34:03 - root - \u001b[32mINFO\u001b[0m - Logging initialized at level INFO [logger.py:202]\n",
            "âœ“ Setup complete!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "from src.monitoring.metrics import MetricsCollector, get_metrics_collector\n",
        "from src.monitoring.logger import setup_logging\n",
        "from src.resources.gpu_manager import get_gpu_manager\n",
        "from src.resources.health_checker import get_health_checker, HealthThresholds\n",
        "from src.scheduler.job_queue import get_job_queue\n",
        "import json\n",
        "import logging\n",
        "\n",
        "setup_logging(level=\"INFO\")  \n",
        "logger = logging.getLogger(__name__)\n",
        "print(\"âœ“ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Metrics Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 20:59:50 - src.resources.gpu_manager - \u001b[33mWARNING\u001b[0m - Failed to initialize pynvml: NVML Shared Library Not Found [gpu_manager.py:78]\n",
            "2025-10-09 20:59:50 - src.resources.gpu_manager - \u001b[33mWARNING\u001b[0m - No CUDA-capable GPUs detected [gpu_manager.py:94]\n",
            "2025-10-09 20:59:50 - src.resources.health_checker - \u001b[32mINFO\u001b[0m - Health checker initialized with 60s interval [health_checker.py:82]\n",
            "2025-10-09 20:59:50 - src.scheduler.priority_manager - \u001b[32mINFO\u001b[0m - Priority manager initialized (fair_share=True, starvation_timeout=3600s) [priority_manager.py:85]\n",
            "2025-10-09 20:59:50 - src.monitoring.metrics - \u001b[32mINFO\u001b[0m - Metrics collector initialized [metrics.py:176]\n",
            "Available Metrics:\n",
            "\n",
            "ðŸ“Š GPU Metrics:\n",
            "  - ml_platform_gpu_utilization_percent\n",
            "  - ml_platform_gpu_temperature_celsius\n",
            "  - ml_platform_gpu_memory_used_bytes\n",
            "  - ml_platform_gpu_memory_total_bytes\n",
            "\n",
            "ðŸ“ˆ Job Metrics:\n",
            "  - ml_platform_jobs_submitted_total (counter)\n",
            "  - ml_platform_jobs_completed_total (counter)\n",
            "  - ml_platform_jobs_failed_total (counter)\n",
            "  - ml_platform_job_duration_seconds (histogram)\n",
            "\n",
            "ðŸ“‹ Queue Metrics:\n",
            "  - ml_platform_queue_depth (gauge)\n",
            "  - ml_platform_active_workers_total (gauge)\n",
            "  - ml_platform_queue_wait_time_seconds (histogram)\n"
          ]
        }
      ],
      "source": [
        "# Get metrics collector\n",
        "metrics = get_metrics_collector()\n",
        "\n",
        "print(\"Available Metrics:\")\n",
        "print(\"\\nðŸ“Š GPU Metrics:\")\n",
        "print(\"  - ml_platform_gpu_utilization_percent\")\n",
        "print(\"  - ml_platform_gpu_temperature_celsius\")\n",
        "print(\"  - ml_platform_gpu_memory_used_bytes\")\n",
        "print(\"  - ml_platform_gpu_memory_total_bytes\")\n",
        "\n",
        "print(\"\\nðŸ“ˆ Job Metrics:\")\n",
        "print(\"  - ml_platform_jobs_submitted_total (counter)\")\n",
        "print(\"  - ml_platform_jobs_completed_total (counter)\")\n",
        "print(\"  - ml_platform_jobs_failed_total (counter)\")\n",
        "print(\"  - ml_platform_job_duration_seconds (histogram)\")\n",
        "\n",
        "print(\"\\nðŸ“‹ Queue Metrics:\")\n",
        "print(\"  - ml_platform_queue_depth (gauge)\")\n",
        "print(\"  - ml_platform_active_workers_total (gauge)\")\n",
        "print(\"  - ml_platform_queue_wait_time_seconds (histogram)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. GPU Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPUs available (CPU mode)\n"
          ]
        }
      ],
      "source": [
        "# Get GPU manager\n",
        "gpu_manager = get_gpu_manager()\n",
        "\n",
        "if gpu_manager.num_gpus > 0:\n",
        "    print(f\"Monitoring {gpu_manager.num_gpus} GPUs:\\n\")\n",
        "    \n",
        "    for gpu_info in gpu_manager.get_all_gpu_info():\n",
        "        print(f\"GPU {gpu_info.id}:\")\n",
        "        print(f\"  Name: {gpu_info.name}\")\n",
        "        print(f\"  Utilization: {gpu_info.utilization:.1f}%\")\n",
        "        print(f\"  Memory: {gpu_info.used_memory/(1024**3):.1f}GB / {gpu_info.total_memory/(1024**3):.1f}GB\")\n",
        "        print(f\"  Temperature: {gpu_info.temperature:.1f}Â°C\")\n",
        "        \n",
        "        # Record metrics\n",
        "        metrics.record_gpu_utilization(gpu_info.id, gpu_info.utilization)\n",
        "        metrics.record_gpu_temperature(gpu_info.id, gpu_info.temperature)\n",
        "        metrics.record_gpu_memory(gpu_info.id, gpu_info.used_memory, gpu_info.total_memory)\n",
        "        print(\"  âœ“ Metrics recorded\\n\")\n",
        "else:\n",
        "    print(\"No GPUs available (CPU mode)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Health Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Health Check Thresholds:\n",
            "  Max GPU Temperature: 85.0Â°C\n",
            "  Max GPU Memory: 95.0%\n",
            "  Check Interval: 60 seconds\n",
            "  Consecutive Failure Threshold: 3\n",
            "\n",
            "Health Summary:\n",
            "  Healthy: 0\n",
            "  Degraded: 0\n",
            "  Unhealthy: 0\n",
            "  Unknown: 0\n"
          ]
        }
      ],
      "source": [
        "# Get health checker\n",
        "health_checker = get_health_checker()\n",
        "thresholds = HealthThresholds()\n",
        "\n",
        "print(\"Health Check Thresholds:\")\n",
        "print(f\"  Max GPU Temperature: {thresholds.max_temperature}Â°C\")\n",
        "print(f\"  Max GPU Memory: {thresholds.max_memory_percent}%\")\n",
        "print(f\"  Check Interval: 60 seconds\")\n",
        "print(f\"  Consecutive Failure Threshold: 3\\n\")\n",
        "\n",
        "# Get health summary\n",
        "summary = health_checker.get_health_summary()\n",
        "\n",
        "print(\"Health Summary:\")\n",
        "print(f\"  Healthy: {summary['healthy']}\")\n",
        "print(f\"  Degraded: {summary['degraded']}\")\n",
        "print(f\"  Unhealthy: {summary['unhealthy']}\")\n",
        "print(f\"  Unknown: {summary['unknown']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Job Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Job submission recorded\n",
            "âœ“ Queue wait time recorded: 45s\n",
            "âœ“ Job completion recorded: 1800s (30 min)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Simulate job lifecycle metrics\n",
        "\n",
        "# Job submitted\n",
        "metrics.record_job_submitted(\n",
        "    user_id=\"demo-user\",\n",
        "    priority=\"MEDIUM\"\n",
        ")\n",
        "print(\"âœ“ Job submission recorded\")\n",
        "\n",
        "# Job started (queue wait time)\n",
        "queue_wait_seconds = 45\n",
        "metrics.record_job_wait_time(priority=\"MEDIUM\", wait_time=queue_wait_seconds)\n",
        "print(f\"âœ“ Queue wait time recorded: {queue_wait_seconds}s\")\n",
        "\n",
        "# Job completed\n",
        "job_duration_seconds = 1800  # 30 minutes\n",
        "metrics.record_job_completed(\n",
        "    user_id=\"demo-user\",\n",
        "    status=\"success\",\n",
        "    duration=job_duration_seconds,\n",
        "    job_type=\"training\",\n",
        "    num_gpus=2\n",
        ")\n",
        "print(f\"âœ“ Job completion recorded: {job_duration_seconds}s (30 min)\\n\")\n",
        "\n",
        "# Get job queue stats\n",
        "job_queue = get_job_queue()\n",
        "stats = job_queue.get_queue_stats()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Structured Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:07:22 - __main__ - \u001b[32mINFO\u001b[0m - Job started [602018948.py:6]\n",
            "2025-10-09 21:07:22 - __main__ - \u001b[32mINFO\u001b[0m - Job progress [602018948.py:16]\n",
            "2025-10-09 21:07:22 - __main__ - \u001b[32mINFO\u001b[0m - Job completed [602018948.py:26]\n",
            "\n",
            "âœ“ Structured logs written (check logs directory)\n",
            "  Format: JSON with timestamp, level, message, context\n"
          ]
        }
      ],
      "source": [
        "# Get logger\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Log with structured context\n",
        "logger.info(\n",
        "    \"Job started\",\n",
        "    extra={\n",
        "        \"job_id\": \"demo-job-001\",\n",
        "        \"user_id\": \"demo-user\",\n",
        "        \"num_gpus\": 2,\n",
        "        \"priority\": \"MEDIUM\"\n",
        "    }\n",
        ")\n",
        "\n",
        "logger.info(\n",
        "    \"Job progress\",\n",
        "    extra={\n",
        "        \"job_id\": \"demo-job-001\",\n",
        "        \"progress\": 50.0,\n",
        "        \"current_loss\": 0.342,\n",
        "        \"epoch\": 1\n",
        "    }\n",
        ")\n",
        "\n",
        "logger.info(\n",
        "    \"Job completed\",\n",
        "    extra={\n",
        "        \"job_id\": \"demo-job-001\",\n",
        "        \"duration_seconds\": 1800,\n",
        "        \"final_loss\": 0.125,\n",
        "        \"status\": \"success\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"\\nâœ“ Structured logs written (check logs directory)\")\n",
        "print(\"  Format: JSON with timestamp, level, message, context\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Alert Rules (Kubernetes)\n",
        "\n",
        "In production, these alerts are defined in `k8s/servicemonitor.yaml`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Production Alert Rules:\n",
            "\n",
            "ðŸ“¢ GPUTemperatureHigh\n",
            "   Condition: gpu_temperature_celsius > 80\n",
            "   For: 5m\n",
            "   Severity: warning\n",
            "   Action: Slack notification + page oncall\n",
            "\n",
            "ðŸ“¢ GPUMemoryExhausted\n",
            "   Condition: (gpu_memory_used / gpu_memory_total) > 0.95\n",
            "   For: 5m\n",
            "   Severity: warning\n",
            "   Action: Slack notification\n",
            "\n",
            "ðŸ“¢ QueueDepthHigh\n",
            "   Condition: queue_depth > 100\n",
            "   For: 10m\n",
            "   Severity: warning\n",
            "   Action: Auto-scale workers\n",
            "\n",
            "ðŸ“¢ WorkerPodDown\n",
            "   Condition: up{job='ml-worker'} == 0\n",
            "   For: 5m\n",
            "   Severity: critical\n",
            "   Action: Page oncall immediately\n",
            "\n",
            "ðŸ“¢ HighJobFailureRate\n",
            "   Condition: (job_failures / job_submissions) > 0.5\n",
            "   For: 10m\n",
            "   Severity: warning\n",
            "   Action: Investigate worker health\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Production Alert Rules:\\n\")\n",
        "\n",
        "alerts = [\n",
        "    {\n",
        "        \"name\": \"GPUTemperatureHigh\",\n",
        "        \"condition\": \"gpu_temperature_celsius > 80\",\n",
        "        \"for\": \"5m\",\n",
        "        \"severity\": \"warning\",\n",
        "        \"action\": \"Slack notification + page oncall\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"GPUMemoryExhausted\",\n",
        "        \"condition\": \"(gpu_memory_used / gpu_memory_total) > 0.95\",\n",
        "        \"for\": \"5m\",\n",
        "        \"severity\": \"warning\",\n",
        "        \"action\": \"Slack notification\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"QueueDepthHigh\",\n",
        "        \"condition\": \"queue_depth > 100\",\n",
        "        \"for\": \"10m\",\n",
        "        \"severity\": \"warning\",\n",
        "        \"action\": \"Auto-scale workers\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"WorkerPodDown\",\n",
        "        \"condition\": \"up{job='ml-worker'} == 0\",\n",
        "        \"for\": \"5m\",\n",
        "        \"severity\": \"critical\",\n",
        "        \"action\": \"Page oncall immediately\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"HighJobFailureRate\",\n",
        "        \"condition\": \"(job_failures / job_submissions) > 0.5\",\n",
        "        \"for\": \"10m\",\n",
        "        \"severity\": \"warning\",\n",
        "        \"action\": \"Investigate worker health\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for alert in alerts:\n",
        "    print(f\"ðŸ“¢ {alert['name']}\")\n",
        "    print(f\"   Condition: {alert['condition']}\")\n",
        "    print(f\"   For: {alert['for']}\")\n",
        "    print(f\"   Severity: {alert['severity']}\")\n",
        "    print(f\"   Action: {alert['action']}\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
